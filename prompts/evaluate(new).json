{
  "MARK_SCHEME_PROMPT": "Generate a numbered Scheme of Evaluation in this format, for each Question X:\n \nQuestion X: \n X: <verbatim question>\n\nAnswer: List the core concepts or their clear equivalents that a full answer must reference, arranged in logical order.\n\nMarking Scheme:\n\n(A marks) A bullet that describes the first key expectation (concept or application) in context.\n\n(B marks) A bullet for the second expectation.\n\n…\n\n(–C marks) A bullet describing any automatic deduction (for example missing required context or terms).\n\nEnsure that:\n\nThe sum of A + B + … equals the total marks for Question X.\n\nAssessment Objectives are stated briefly at the top (for example, Conceptual Understanding, Application & Problem-Solving, Relevance & Specificity).\n\nPlease include a Notes section with each question’s mark scheme:\n\nOpen-ended questions: any one comprehensive, context-relevant answer can earn full marks.\n\nMinor calculation errors should not cost any marks if reasoning is sound.\n\nCoding expectations or no alternative approaches rules when applicable.\n\nBullets must call out the concept or context the student needs to cover, but need not use verbatim phrasing—equivalent meaning is fine. Provide marks liberally if the student shows rich understanding even if not using the right terms.\n\nPlease create a mark scheme for the following questions: ${questions_text}",  
  "TOPIC_ANALYSIS_PROMPT": "Analyze the following exam questions and group them into core academic topics.\nEnsure each question is mapped to exactly one topic. Use clear, standardized topic names.\nGenerate topics based solely on the content of the questions, without adding unrelated or placeholder topics.\n\nQuestions:\n{questions}\n\nReturn a valid JSON object with:\n- \"topics\": array of topic names derived from the questions\n- \"question_mapping\": {{\"question_number\": \"topic_name\"}}\n\nReturn ONLY a valid JSON object, no explanations or markdown.",
  "EVALUATION_PROMPT": "Evaluate the following student answers based on the provided mark scheme.\n\nQuestion {q_num} ({marks} marks): {question_text}\nCorrect Answer: {correct_answer}\nMarking Scheme: {marking_scheme}\n\nStudent's Answer:\n{student_text}\nAssociated Diagrams: {image_count} diagram(s) provided. Image data is included in the message if available.\n\nEvaluation Guidelines:\n1. Text Evaluation: Compare the student's textual answer to the correct answer and marking scheme. Award marks based on accuracy, completeness, and relevance. Even if the answer contrasts with what is given in the mark scheme, if the student has given a sound explanation to the answer, then you could give marks based on your judgement. Always when you are torn between giving a lower and a higher score, always give the higher one. If no text and image are provided, award 0 marks.\n2. Diagram Evaluation:\n   - If image data is provided in the message, analyze the image(s) for relevance, accuracy, and completeness relative to the question and correct answer.\n   - Check for correct annotations, labels, and clarity in the image(s).\n   - If only diagrams are provided (no text), base the entire score on the diagram's accuracy, completeness, and alignment with the marking scheme.\n   - If image_count > 0 but no image data is provided, note this in the feedback and base the score solely on the textual answer.\n   - Award partial marks for partially correct diagrams, considering key elements specified in the marking scheme.\n   - If no diagrams are provided (image_count = 0), evaluate based on textual content alone.\n3. Combined Assessment: Integrate text and diagram evaluations to provide a cohesive score. If only diagrams are present and valid, rely solely on diagram evaluation. If diagrams are indicated but cannot be analyzed, rely on textual evaluation.\n4. Fairness: Ensure consistent scoring aligned with the marking scheme. Diagrams should be weighted appropriately if specified in the marking scheme.\n5. Missing Answers: If no relevant textual or diagrammatic answer is provided, award 0 marks.\n\n[\n  {{\n    \"question_number\": \"{q_num}\",\n    \"sub_questions\": [\n      {{\n        \"sub_question_id\": \"X.Y\",\n        \"score\": \"Z/W\",\n        \"text_feedback\": \"Detailed analysis of the textual answer or 'No text provided' if none avoid stating that certain deficiencies in the answer are all right like 'calculation mistakes do not matter', but highlight these deficiencies in the improvement suggestions. There is no need to explain the partial marks breakdown in the feedback or refer to the marking scheme.\",\n        \"improvement_suggestions\": \"Specific suggestions for improvement, including possible better ways.\"\n      }}\n    ],\n    \"total_score\": \"X/{marks}\",\n    \"total_feedback\": \"Overall feedback for the question, summarizing performance across sub-questions.\"\n  }}\n]\n\nEnsure the sum of sub-question scores equals the total_score for the question. Return only a valid JSON array containing one object per question, with no additional text or markdown."
}

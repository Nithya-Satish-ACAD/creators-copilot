{
  "ms_prompt": "Generate a numbered Scheme of Evaluation in this format, for the given Question X:\n\nQuestion X: <verbatim question>\n\nAnswer template: List the core concepts or their clear equivalents that a full answer must reference, arranged in logical order.\n\nMarking Scheme:\n\n(A marks) A bullet that describes the first key expectation (concept or application) in context.\n\n(B marks) A bullet for the second expectation.\n\n…\n\n(–C marks) A bullet describing any automatic deduction (for example missing required context or terms).\n\nEnsure that:\n\nThe sum of A + B + … equals the total marks for Question X. Keep A, B, C, … as integers only. Try to maximize the number of bullets.\n\nAssessment Objectives are stated briefly at the top (for example, Conceptual Understanding, Application & Problem-Solving, Relevance & Specificity).\n\nPlease include a Notes section with each question’s mark scheme:\n\nOpen-ended questions: any one comprehensive, context-relevant answer can earn full marks.\n\nMinor calculation errors should not cost any marks if reasoning is sound.\n\nBullets must call out the concept or context the student needs to cover, but need not use verbatim phrasing—equivalent meaning is fine. Provide marks liberally if the student shows rich understanding even if not using the right terms.\n\n***  \n⚠️ **IMPORTANT**: your **only** output must be a single JSON object (for single questions) or a JSON array of such objects (for batch), each with these keys:\n```json\n{\n  \"question_number\": <int>,\n  \"answer_template\": \"<string>\",\n  \"marking_scheme\": [\"<bullet 1>\", \"<bullet 2>\", …],\n  \"deductions\": [\"<bullet 1>\", …],\n  \"notes\": \"<string>\"\n}\n```\n\nFor batch requests, append the following before listing the questions:\n\nNow generate a JSON array of mark-scheme objects for each of the following questions.  \nEach object must have the same keys listed above.  \n\nQuestions:  \n{questions_json}",
  "topic_prompt": "Analyze the following exam questions and group them into 5-7 core academic topics.\nEnsure each question is mapped to exactly one topic. Use clear, standardized topic names.\n\nQuestions:\n{json.dumps(questions, indent=2)}\n\nFormat response EXACTLY as a valid JSON object with:\n- \"topics\": array of 5-7 topic names\n- \"question_mapping\": {\"question_number\": \"topic_name\"}\n\nIMPORTANT: Return only a valid JSON object and nothing else. No explanations, no markdown formatting.",
  "topic_retry_prompt": "Analyze the following exam questions and group them into 5-7 core academic topics.\n\nQuestions:\n{json.dumps(questions, indent=2)}\n\nYOUR RESPONSE MUST BE A VALID JSON OBJECT ONLY - NO TEXT BEFORE OR AFTER.\nDO NOT USE MARKDOWN CODE BLOCKS OR BACKTICKS.\n\nThe JSON should have this exact structure:\n{\n  \"topics\": [\"Topic1\", \"Topic2\", \"Topic3\", \"Topic4\", \"Topic5\"],\n  \"question_mapping\": {\"1\": \"Topic1\", \"2\": \"Topic2\", ...}\n}",
  "eval_prompt": "Evaluate all student answers below against their Schemes of Evaluation.\n\nEach item in the data has:\n- question_number\n- marks\n- question_text\n- student_text\n- answer_template\n- marking_scheme\n- deductions\n- notes\n\nEvaluation Guidelines:\n- Compare the student's answer to the answer_template and marking_scheme.\n- Award marks based on accuracy, completeness, and relevance.\n- If the answer deviates but is sound, favor the higher score.\n- If no text is provided, award 0 marks.\n\nRequired Output:\nReturn a JSON array of objects with keys:\n  - question_number\n  - score\n  - text_feedback\n  - improvement_suggestions\n\nData:\n{payload_json}"
}
